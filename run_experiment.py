import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import os
import pandas as pd
import numpy as np

# Configuraci√≥n b√°sica
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
latent_dim = 64
batch_size = 128
epochs = 20
save_interval = 4

# Crear directorio para guardar resultados
os.makedirs("results", exist_ok=True)

# DataFrame para guardar las p√©rdidas
losses_df = pd.DataFrame(columns=['epoch', 'discriminator_loss', 'generator_loss'])

# Transformaci√≥n y carga de datos MNIST
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])  # escala de -1 a 1
])

dataloader = DataLoader(
    datasets.MNIST(root="./data", train=True, download=True, transform=transform),
    batch_size=batch_size,
    shuffle=True
)

# Generador
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 784),
            nn.Tanh()  # salida en el rango [-1, 1]
        )

    def forward(self, z):
        return self.net(z)

# Discriminador
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(784, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

# Inicializaci√≥n de modelos y optimizadores
G = Generator().to(device)
D = Discriminator().to(device)

loss_fn = nn.BCELoss()
optimizer_G = optim.Adam(G.parameters(), lr=0.0002)
optimizer_D = optim.Adam(D.parameters(), lr=0.0002)

# Funci√≥n para generar y guardar muestras
def save_generated_samples(epoch, generator, latent_dim=64, device=device):
    generator.eval()
    with torch.no_grad():
        z = torch.randn(16, latent_dim).to(device)
        samples = generator(z).reshape(-1, 1, 28, 28).cpu()
        
        # Crear una cuadr√≠cula 4x4 de im√°genes
        fig, axes = plt.subplots(4, 4, figsize=(8, 8))
        for i in range(16):
            row, col = i // 4, i % 4
            axes[row, col].imshow(samples[i].squeeze(), cmap='gray')
            axes[row, col].axis('off')
        
        plt.suptitle(f'Muestras generadas - √âpoca {epoch}')
        plt.tight_layout()
        
        # Guardar la imagen
        plt.savefig(f'results/generated_epoch_{epoch:03d}.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"Im√°genes guardadas en results/generated_epoch_{epoch:03d}.png")

# Entrenamiento
for epoch in range(epochs):
    for real_imgs, _ in dataloader:
        real_imgs = real_imgs.view(-1, 784).to(device)
        batch_size = real_imgs.size(0)

        # üè∑Ô∏è Etiquetas reales y falsas
        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        # ---------------------
        # 1. Entrenar el Discriminador
        # ---------------------
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_imgs = G(z)

        real_loss = loss_fn(D(real_imgs), real_labels)
        fake_loss = loss_fn(D(fake_imgs.detach()), fake_labels)
        d_loss = real_loss + fake_loss

        optimizer_D.zero_grad()
        d_loss.backward()
        optimizer_D.step()

        # ---------------------
        # 2. Entrenar el Generador
        # ---------------------
        z = torch.randn(batch_size, latent_dim).to(device)
        fake_imgs = G(z)
        g_loss = loss_fn(D(fake_imgs), real_labels)  # queremos que D piense que son reales

        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()

    print(f"√âpoca {epoch+1}/{epochs} | P√©rdida D: {d_loss.item():.4f} | P√©rdida G: {g_loss.item():.4f}")
    
    # Generar y guardar muestras cada 10 √©pocas
    if (epoch + 1) % save_interval == 0:
        save_generated_samples(epoch + 1, G)

    # Guardar p√©rdidas en el DataFrame
    new_row = pd.DataFrame({'epoch': [epoch + 1], 'discriminator_loss': [d_loss.item()], 'generator_loss': [g_loss.item()]})
    losses_df = pd.concat([losses_df, new_row], ignore_index=True)

# Mostrar resultados finales
def show_generated():
    G.eval()
    with torch.no_grad():
        z = torch.randn(16, latent_dim).to(device)
        samples = G(z).reshape(-1, 1, 28, 28).cpu()
        grid = torch.cat([s.squeeze(0) for s in samples], dim=1)
        plt.imshow(grid, cmap='gray')
        plt.axis("off")
        plt.title("Muestras generadas finales")
        plt.show()

print("\nEntrenamiento completado! Mostrando resultados finales...")
show_generated()

# Guardar DataFrame de p√©rdidas
losses_df.to_csv('results/losses.csv', index=False)

# Funci√≥n para generar gr√°ficos de p√©rdidas
def plot_losses(losses_df):
    plt.figure(figsize=(15, 5))
    
    # Gr√°fico 1: P√©rdidas individuales
    plt.subplot(1, 3, 1)
    plt.plot(losses_df['epoch'], losses_df['discriminator_loss'], label='Discriminador', color='blue')
    plt.plot(losses_df['epoch'], losses_df['generator_loss'], label='Generador', color='red')
    plt.xlabel('√âpoca')
    plt.ylabel('P√©rdida')
    plt.title('Evoluci√≥n de las P√©rdidas')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Gr√°fico 2: P√©rdida del Discriminador
    plt.subplot(1, 3, 2)
    plt.plot(losses_df['epoch'], losses_df['discriminator_loss'], color='blue', linewidth=2)
    plt.xlabel('√âpoca')
    plt.ylabel('P√©rdida del Discriminador')
    plt.title('P√©rdida del Discriminador')
    plt.grid(True, alpha=0.3)
    
    # Gr√°fico 3: P√©rdida del Generador
    plt.subplot(1, 3, 3)
    plt.plot(losses_df['epoch'], losses_df['generator_loss'], color='red', linewidth=2)
    plt.xlabel('√âpoca')
    plt.ylabel('P√©rdida del Generador')
    plt.title('P√©rdida del Generador')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/loss_evolution.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("Gr√°ficos de p√©rdidas guardados en results/loss_evolution.png")

# Funci√≥n para generar gr√°fico de convergencia
def plot_convergence(losses_df):
    plt.figure(figsize=(10, 6))
    
    # Calcular la diferencia entre p√©rdidas para ver convergencia
    loss_diff = np.abs(losses_df['discriminator_loss'] - losses_df['generator_loss'])
    
    plt.plot(losses_df['epoch'], loss_diff, color='green', linewidth=2)
    plt.xlabel('√âpoca')
    plt.ylabel('|P√©rdida D - P√©rdida G|')
    plt.title('Convergencia del GAN\n(Diferencia entre p√©rdidas)')
    plt.grid(True, alpha=0.3)
    
    # Agregar l√≠nea horizontal en y=0.5 para referencia
    plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Umbral de convergencia')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('results/convergence.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("Gr√°fico de convergencia guardado en results/convergence.png")

# Generar todos los gr√°ficos
print("\nGenerando gr√°ficos de an√°lisis...")
plot_losses(losses_df)
plot_convergence(losses_df)

print(f"\nResumen del entrenamiento:")
print(f"P√©rdida final del Discriminador: {losses_df['discriminator_loss'].iloc[-1]:.4f}")
print(f"P√©rdida final del Generador: {losses_df['generator_loss'].iloc[-1]:.4f}")
print(f"P√©rdidas guardadas en: results/losses.csv")
