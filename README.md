## Redes adversarias generativas (GANs)

### 1. **Objetivo General**

Las GANs buscan aprender la distribuci√≥n de probabilidad de los datos reales $p_{\text{data}}(x)$, usando una red generadora $G(z)$, que transforma ruido $z \sim p_z(z)$ (por ejemplo, una gaussiana) en muestras sint√©ticas $G(z) \sim p_g(x)$.

---

### 2. **Componentes del Modelo**

#### üîπ Generador $G(z; \theta_g)$

* Toma como entrada una variable aleatoria $z \sim p_z(z)$, t√≠picamente ruido gaussiano o uniforme.
* Devuelve una muestra sint√©tica $x' = G(z)$.
* Tiene par√°metros $\theta_g$ que se actualizan para que $p_g(x) \approx p_{\text{data}}(x)$.

#### üîπ Discriminador $D(x; \theta_d)$

* Es una funci√≥n que estima la probabilidad de que una muestra $x$ provenga del conjunto de datos reales.
* Devuelve un valor en $[0, 1]$:

  $$
  D(x) = \mathbb{P}(x \text{ es real})
  $$
* Se entrena para maximizar su capacidad de distinguir entre datos reales y sint√©ticos.

---

### 3. **Juego Minimax (Formulaci√≥n de Teor√≠a de Juegos)**

La GAN se entrena como un **juego de dos jugadores** en el que el generador quiere enga√±ar al discriminador, y este √∫ltimo quiere detectarlo:

$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

#### üîç Interpretaci√≥n:

* $\mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)]$: El discriminador es recompensado por asignar alta probabilidad a los datos reales.
* $\mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]$: Es recompensado por asignar baja probabilidad a datos generados.
* El generador quiere minimizar la segunda expectativa, es decir, quiere que $D(G(z)) \to 1$.

---

### 4. **Entrenamiento Alternado**

1. **Actualizar $D$**:
   Mantener $G$ fijo, y maximizar:

   $$
   \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
   $$

2. **Actualizar $G$**:
   Mantener $D$ fijo, y minimizar:

   $$
   \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
   $$

   O mejor (para evitar gradientes d√©biles cuando $D(G(z)) \approx 0$):

   $$
   \text{Usar: } \mathbb{E}_{z \sim p_z}[-\log D(G(z))] \quad \text{(Heuristic non-saturating loss)}
   $$


### 5. **GAN Lab**

En el siguiente enlace se encuentra disponible una ilustraci√≥n interactiva del funcionamiento de las (GANs): https://poloclub.github.io/ganlab/

Aqu√≠ tienes una **secci√≥n para agregar al final del `README.md`** con la descripci√≥n clara de la tarea para estudiantes del curso de Redes Neuronales y Aprendizaje Profundo. Est√° redactada de forma acad√©mica, con √©nfasis en la comprensi√≥n del entrenamiento de la GAN y en el desarrollo de una interfaz gr√°fica.

---

### 6. **Tarea: An√°lisis de Din√°mica de Entrenamiento y Desarrollo de Interfaz Interactiva**

A partir del script `run_experiment.py`, los estudiantes deben desarrollar una actividad pr√°ctica que consta de dos partes:

#### Parte 1: An√°lisis de la din√°mica de entrenamiento

1. Ejecutar el script `run_experiment.py` para entrenar una red generativa adversaria sobre el conjunto de datos MNIST.
2. Analizar la evoluci√≥n de las p√©rdidas del generador y del discriminador a lo largo de las √©pocas.
3. Realizar un **reporte t√©cnico** que incluya:

   * Gr√°ficos de las p√©rdidas y su interpretaci√≥n.
   * Discusi√≥n sobre el equilibrio entre generador y discriminador.
   * Visualizaci√≥n de las im√°genes generadas en distintas √©pocas.
   * Reflexi√≥n sobre la convergencia y estabilidad del modelo.

#### Parte 2: Interfaz gr√°fica con generaci√≥n y reconocimiento

Desarrollar una **interfaz gr√°fica** que incluya al menos dos funcionalidades:

* **Generar d√≠gito aleatorio**: Un bot√≥n que, al hacer clic, utilice el generador entrenado para crear una nueva imagen de d√≠gito.
* **Reconocer d√≠gito generado**: Otro bot√≥n que aplique un clasificador entrenado (por ejemplo, un modelo preentrenado en PyTorch) para identificar autom√°ticamente qu√© d√≠gito representa la imagen generada. Opcionalmente, se pueden mostrar los valores de confianza (probabilidades) del clasificador.